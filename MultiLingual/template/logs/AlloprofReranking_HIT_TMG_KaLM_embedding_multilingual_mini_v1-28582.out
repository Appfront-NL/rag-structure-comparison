/var/scratch/tkl206/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
The `batch_size` argument is deprecated and will be removed in the next release. Please use `encode_kwargs = {'batch_size': ...}` to set the batch size instead.
Model prompts are not in the expected format. Ignoring them.
─────────────────────────────── Selected tasks  ────────────────────────────────
Reranking
    - AlloprofReranking, s2p


A total of 22751/25039 duplicate texts were found during encoding. Only encoding unique text and duplicating embeddings across.
slurmstepd-node002: error: *** JOB 28582 ON node002 CANCELLED AT 2025-06-03T17:53:43 ***
