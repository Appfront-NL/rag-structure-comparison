/var/scratch/tkl206/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Some weights of the model checkpoint at NovaSearch/jasper_en_vision_language_v1 were not used when initializing JasperVL: ['vision_model.vision_model.embeddings.patch_embedding.bias', 'vision_model.vision_model.embeddings.patch_embedding.weight', 'vision_model.vision_model.embeddings.position_embedding.weight', 'vision_model.vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.24.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.24.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.24.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.24.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.24.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.24.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.24.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.24.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.24.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.24.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.24.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.24.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.24.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.24.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.24.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.24.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.25.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.25.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.25.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.25.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.25.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.25.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.25.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.25.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.25.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.25.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.25.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.25.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.25.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.25.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.25.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.25.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.26.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.26.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.26.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.26.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.26.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.26.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.26.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.26.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.26.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.26.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.26.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.26.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.26.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.26.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.26.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.26.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.vision_model.head.attention.in_proj_bias', 'vision_model.vision_model.head.attention.in_proj_weight', 'vision_model.vision_model.head.attention.out_proj.bias', 'vision_model.vision_model.head.attention.out_proj.weight', 'vision_model.vision_model.head.layernorm.bias', 'vision_model.vision_model.head.layernorm.weight', 'vision_model.vision_model.head.mlp.fc1.bias', 'vision_model.vision_model.head.mlp.fc1.weight', 'vision_model.vision_model.head.mlp.fc2.bias', 'vision_model.vision_model.head.mlp.fc2.weight', 'vision_model.vision_model.head.probe', 'vision_model.vision_model.post_layernorm.bias', 'vision_model.vision_model.post_layernorm.weight']
- This IS expected if you are initializing JasperVL from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing JasperVL from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
─────────────────────────────── Selected tasks  ────────────────────────────────
Reranking
    - WebLINXCandidatesReranking, p2p
    - AlloprofReranking, s2p
    - WikipediaRerankingMultilingual, s2p, multilingual 11 / 16 Subsets


Retrieval
    - AlloprofRetrieval, s2p
    - BelebeleRetrieval, s2p, multilingual 73 / 376 Subsets
    - StatcanDialogueDatasetRetrieval, s2p, multilingual 2 / 2 Subsets
    - WikipediaRetrievalMultilingual, s2p, multilingual 11 / 16 Subsets


STS
    - STS17, s2s, multilingual 7 / 11 Subsets
    - STSES, s2s


Batches:   0%|          | 0/19 [00:00<?, ?it/s]Batches:   0%|          | 0/19 [00:00<?, ?it/s]
Error while evaluating AlloprofRetrieval: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasGemmStridedBatchedEx(handle, opa, opb, (int)m, (int)n, (int)k, (void*)&falpha, a, CUDA_R_16BF, (int)lda, stridea, b, CUDA_R_16BF, (int)ldb, strideb, (void*)&fbeta, c, CUDA_R_16BF, (int)ldc, stridec, (int)num_batches, compute_type, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`
Traceback (most recent call last):
  File "/home/tkl206/rag-structure-comparison/MultiLingual/MultilingualTest.py", line 37, in <module>
    results = evaluation.run(model, output_folder="ML-results-test", return_all_scores=True)
  File "/var/scratch/tkl206/anaconda3/envs/myenv/lib/python3.10/site-packages/mteb/evaluation/MTEB.py", line 672, in run
    raise e
  File "/var/scratch/tkl206/anaconda3/envs/myenv/lib/python3.10/site-packages/mteb/evaluation/MTEB.py", line 625, in run
    results, tick, tock = self._run_eval(
  File "/var/scratch/tkl206/anaconda3/envs/myenv/lib/python3.10/site-packages/mteb/evaluation/MTEB.py", line 307, in _run_eval
    results = task.evaluate(
  File "/var/scratch/tkl206/anaconda3/envs/myenv/lib/python3.10/site-packages/mteb/abstasks/AbsTaskRetrieval.py", line 344, in evaluate
    scores[hf_subset] = self._evaluate_subset(
  File "/var/scratch/tkl206/anaconda3/envs/myenv/lib/python3.10/site-packages/mteb/abstasks/AbsTaskRetrieval.py", line 353, in _evaluate_subset
    results = retriever(corpus, queries)
  File "/var/scratch/tkl206/anaconda3/envs/myenv/lib/python3.10/site-packages/mteb/evaluation/evaluators/RetrievalEvaluator.py", line 485, in __call__
    return self.retriever.search(
  File "/var/scratch/tkl206/anaconda3/envs/myenv/lib/python3.10/site-packages/mteb/evaluation/evaluators/RetrievalEvaluator.py", line 123, in search
    query_embeddings = self.model.encode(
  File "/var/scratch/tkl206/anaconda3/envs/myenv/lib/python3.10/site-packages/mteb/evaluation/evaluators/RetrievalEvaluator.py", line 423, in encode
    return self.model.encode(
  File "/var/scratch/tkl206/anaconda3/envs/myenv/lib/python3.10/site-packages/mteb/models/jasper_models.py", line 50, in encode
    embeddings = self.model.encode(
  File "/var/scratch/tkl206/anaconda3/envs/myenv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py", line 685, in encode
    out_features = self.forward(features, **kwargs)
  File "/var/scratch/tkl206/anaconda3/envs/myenv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py", line 758, in forward
    input = module(input, **module_kwargs)
  File "/var/scratch/tkl206/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/var/scratch/tkl206/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/tkl206/.cache/huggingface/modules/transformers_modules/NovaSearch/jasper_en_vision_language_v1/d6330ce98f8a0d741e781df845904c9484f00efa/custom_st.py", line 40, in forward
    sentence_embedding = self.auto_model(**trans_features, **kwargs)[
  File "/var/scratch/tkl206/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/var/scratch/tkl206/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/tkl206/.cache/huggingface/modules/transformers_modules/NovaSearch/jasper_en_vision_language_v1/d6330ce98f8a0d741e781df845904c9484f00efa/modeling_jasper_vl.py", line 1160, in forward
    last_hidden_state = self.model(
  File "/var/scratch/tkl206/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/var/scratch/tkl206/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/tkl206/.cache/huggingface/modules/transformers_modules/NovaSearch/jasper_en_vision_language_v1/d6330ce98f8a0d741e781df845904c9484f00efa/modeling_jasper_vl.py", line 1071, in forward
    layer_outputs = decoder_layer(
  File "/var/scratch/tkl206/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/var/scratch/tkl206/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/tkl206/.cache/huggingface/modules/transformers_modules/NovaSearch/jasper_en_vision_language_v1/d6330ce98f8a0d741e781df845904c9484f00efa/modeling_jasper_vl.py", line 770, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/var/scratch/tkl206/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/var/scratch/tkl206/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/tkl206/.cache/huggingface/modules/transformers_modules/NovaSearch/jasper_en_vision_language_v1/d6330ce98f8a0d741e781df845904c9484f00efa/modeling_jasper_vl.py", line 279, in forward
    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasGemmStridedBatchedEx(handle, opa, opb, (int)m, (int)n, (int)k, (void*)&falpha, a, CUDA_R_16BF, (int)lda, stridea, b, CUDA_R_16BF, (int)ldb, strideb, (void*)&fbeta, c, CUDA_R_16BF, (int)ldc, stridec, (int)num_batches, compute_type, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`
